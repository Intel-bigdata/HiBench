# Two data sets(text and numeric) are available, app argument indicates to use which
#app=micro-sketch      #use text dataset, avg record size: 60 bytes
#app=micro-statistics  #use numeric dataset, avg record size: 200 bytes
hibench.streamingbench.app        micro-sketch

# Text dataset can be scaled in terms of record size
hibench.streamingbench.prepare.textdataset_recordsize_factor 

# Two modes of generator: push,periodic
# Push means to send data to kafka cluster as fast as it could
# Periodic means sending data according to sending rate specification
#hibench.streamingbench.prepare.mode     push
hibench.streamingbench.prepare.mode     periodic

# Under push mode: number of total records that will be generated
hibench.streamingbench.prepare.push.records	900000000

# Following three params are under periodic mode
# Bytes to push per interval
hibench.streamingbench.prepare.periodic.recordPerInterval  600000

# Interval time (in ms)
hibench.streamingbench.prepare.periodic.intervalSpan 5000

# Total round count of data send
hibench.streamingbench.prepare.periodic.totalRound   100

# zookeeper host:port of kafka cluster

#example: hostname:9092
hibench.streamingbench.zookeeper.host	HOSTNAME:HOSTPORT

#Parallel config
# number of nodes that will receive kafka input
hibench.streamingbench.receiver_nodes	4

###############
#Benchmark args
#Note to ensure benchName to be consistent with datagen type. Numeric data for statistics and text data for others
# available benchname: identity sample project grep wordcount distinctcount statistics

hibench.streamingbench.benchname	identity

#common args
# the topic that spark will receive input data
hibench.streamingbench.topic_name	${hibench.streamingbench.benchname}

# Spark stream batch interval (in seconds)
hibench.streamingbench.batch_interval	10

# consumer group of the spark consumer for kafka
hibench.streamingbench.consumer_group	HiBench

# expected number of records to be processed
hibench.streamingbench.record_count	900000000

#sketch/distinctcount/statistics arg
# the field index of the record that will be extracted
hibench.streamingbench.field_index	1

#sketch/wordcount/distinctcount/statistics arg
# the seperator between fields of a single record
hibench.streamingbench.separator	\\s+

#sample arg
# probability that a record will be taken as a sample
hibench.streamingbench.prob		0.1

#grep arg
# the substring that will be checked to see if contained in a record
hibench.streamingbench.pattern		the

#common arg
# indicate RDD storage level. 
# 1 for memory only 1 copy. Others for default mem_disk_ser 2 copies 
hibench.streamingbench.copies		2

# indicate whether to test the write ahead log new feature
# set true to test WAL feature
hibench.streamingbench.testWAL		false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty
hibench.streamingbench.checkpoint_path	

#common arg
# indicate whether in debug mode for correctness verfication
hibench.streamingbench.debug		false

# whether to use direct approach or not ( sparkstreaming only )
hibench.streamingbench.direct_mode	true	

# Kafka broker lists, used for direct mode, written in mode "host:port,host:port,..."

# example: hostname:9092
hibench.streamingbench.brokerList	HOSTNAME:HOSTPORT

hibench.streamingbench.broker_list_with_quote	"${hibench.streamingbench.brokerList}"

# storm bench conf

# STORM_BIN_HOME
hibench.streamingbench.storm.home	/PATH/TO/STORM/HOME

# Kafka home
hibench.streamingbench.kafka.home	/PATH/TO/KAFKA/HOME


#Cluster config
# nimbus of storm cluster
hibench.streamingbench.storm.nimbus		HOSTNAME_OF_STORM
hibench.streamingbench.storm.nimbusAPIPort	6627

# time interval to contact nimbus to judge if finished
hibench.streamingbench.storm.nimbusContactInterval	10


#Parallel config

# number of workers of Storm. Number of most bolt threads is also equal to this param.
hibench.streamingbench.storm.worker_count	12

# number of kafka spout threads of Storm
hibench.streamingbench.storm.spout_threads	12

# number of bolt threads altogether
hibench.streamingbench.storm.bolt_threads	12

# kafka arg indicating whether to read data from kafka from the start or go on to read from last position
hibench.streamingbench.storm.read_from_start	true

# whether to turn on ack
hibench.streamingbench.storm.ackon		true

# Added for default rules:
hibench.streamingbench.jars		${hibench.streamingbench.sparkbench.jar}
hibench.streamingbench.sparkbench.jar	${hibench.home}/src/streambench/sparkbench/target/streaming-bench-spark_0.1-5.0-SNAPSHOT-${hibench.spark.version}-jar-with-dependencies.jar
hibench.streamingbench.stormbench.jar	${hibench.home}/src/streambench/stormbench/target/streaming-bench-storm-0.1-SNAPSHOT-jar-with-dependencies.jar
hibench.streamingbench.gearpump.jar ${hibench.home}/src/streambench/gearpumpbench/target/streaming-bench-gearpump-5.0-SNAPSHOT-jar-with-dependencies.jar
hibench.streamingbench.datagen.jar      ${hibench.home}/src/streambench/datagen/target/datagen-0.0.1-jar-with-dependencies.jar
hibench.streamingbench.storm.bin	${hibench.streamingbench.storm.home}/bin
hibench.streamingbench.zkhelper.jar      ${hibench.home}/src/streambench/zkHelper/target/streaming-bench-zkhelper-0.1-SNAPSHOT-jar-with-dependencies.jar

# default path setting for store of data1 & data2
hibench.streamingbench.datagen.dir      ${hibench.hdfs.data.dir}/Streaming

# partition size settings
hibench.streamingbench.partitions	1

## gearpump bench conf
hibench.streamingbench.gearpump.home  /home/huafengw/workspace/incubator-gearpump/output/target/pack
hibench.streamingbench.gearpump.executors  1
hibench.streamingbench.gearpump.parallelism  1
